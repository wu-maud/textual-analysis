{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import re\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "'''type_choose decides the following operation are aimed at mda data or audit data'''\n",
    "type_choose=\"mda\"\n",
    "\n",
    "if type_choose !=\"mda\" and type_choose !=\"audit\":\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A lot of sentences are depareted with page header such as \"XXXX年年度报告\" and \"XX/XXXX\", which lead to a great deal of loss of high quality sentences,this step are aim to recognize and clean the noise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read_path leads to the raw_data,\n",
    "write_path leads to the data without noise,\n",
    "parent_start and end_string are used to recognize the aimed file in raw data.\n",
    "\n",
    "both mda sentence and audit sentence need this step.\n",
    "so if you want to deal with mda_data,use the first four line codes,\n",
    "if you want to deal with audit_data,use the next four line codes.\n",
    "\n",
    "'''\n",
    "\n",
    "read_path='../../raw_data/mda_data'\n",
    "write_path='../../mid_data/mda_clean1'\n",
    "parent_start=-15\n",
    "end_string='pdf.txt_total.md'\n",
    "if type_choose==\"audit\":\n",
    "    read_path='../../raw_data/audit_data'\n",
    "    write_path='../../mid_data/audit_clean1'\n",
    "    end_string='pdf.txt_part_3.md'\n",
    "\n",
    "'''relualar expression'''\n",
    "pattern=re.compile(r\"(\\d+(\\n+)(.*)\\d+(\\s)?年年度报告全文\\n)|(\\d+(\\s)?年年度报告全文\\n)|(\\d+(\\n+)(.*)\\d+(\\s)?年年度报告\\n)|(\\d+(\\s)?年年度报告\\n)|((\\s)*\\d+\\s\\/\\s218\\s2014\\s年年度报告)|(\\n\\d+\\s/)\")\n",
    "\n",
    "'''build blank file if write_path doesn't exist'''\n",
    "if not os.path.exists(write_path):\n",
    "    os.makedirs(write_path)\n",
    "    \n",
    "    \n",
    "for parent,dirnames,filenames in os.walk(read_path):       \n",
    "    filenumber=1\n",
    "    for filename in filenames:\n",
    "        \n",
    "        if filename.endswith(end_string):\n",
    "           \n",
    "            main_path=write_path+'/'+ parent[parent_start:] \n",
    "            \n",
    "            '''rebuild the file tree of reading_path'''\n",
    "            if not os.path.exists(main_path):\n",
    "                os.makedirs(main_path)\n",
    "                \n",
    "            fwrite = open(os.path.join(main_path,filename),'w',encoding='utf-8')\n",
    "            content=open(os.path.join(parent,filename),'r',encoding='utf-8').read()\n",
    "            \n",
    "            '''delete sentence noise with string replacement'''\n",
    "            while(re.search(pattern,content)!=None):\n",
    "                content=re.sub( pattern,\"\", content)\n",
    "            fwrite.write(content)\n",
    "            fwrite.close()    \n",
    "            \n",
    "            filenumber=filenumber+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Clean: Sentences Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 切句子规则：\n",
    "- 1.按行读取句子。如果句子格式是大标题，例如：“第X节”，“（一）、”，“一、” 等格式则弃而不用。考虑到诸如“1、”，“a、”这样的小标题后跟的可能是具体条例，因此不删除。\n",
    "- 2.如果该行数据里不出现“。”，则删除。因为可能是乱码、表格数据等。\n",
    "- 3.某行数据里若存在“。”，则按照句号进行分句处理。\n",
    "- 4.所有句子要求长度超过8，如果没有超过则删除。则删掉诸如“由下表所示：”等无意义短句。\n",
    "#### 其余处理：\n",
    "- 1.抽取股票id，年份信息，给每份年报拟年报id，给每条句子赋值句子id，统计每份年报的句子总数量。\n",
    "- 2.准备两种csv文件，第一个csv文件 按照：股票id，年份，年报id，句子id，积极消极待标注区，句子内容写入。另一个统计csv文件写入 股票id，年份，年报id，年报句子总数（不包括审计文件），文件路径。\n",
    "- 3.处理统计csv时，出现原始数据里由于公司更名或者重复提交年报导致的同一股票代码同一年份多篇年报的现象，这些年报需要特殊标记然后另外处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read_path leads to the data after first cleaned ,\n",
    "write_path leads to the csv file\n",
    "\n",
    "both mda sentence and audit sentence need this step.\n",
    "so if you want to deal with mda_data,use the first two line codes,\n",
    "if you want to deal with audit_data,use the next two line codes.\n",
    "\n",
    "'''\n",
    "read_path='../../mid_data/mda_clean1'\n",
    "write_path='../../mid_data/mda_clean2/'\n",
    "if type_choose==\"audit\":\n",
    "    read_path='../../mid_data/audit_clean1'\n",
    "    write_path='../../mid_data/audit_clean2/'\n",
    "\n",
    "pattern=re.compile(r\"第\\S节|[(]?[一二三四五六七八九十]+[)]?\\、|\\([一二三四五六七八九十]+\\)\")\n",
    "\n",
    "\n",
    "if not os.path.exists(write_path):\n",
    "    os.makedirs(write_path)\n",
    "    \n",
    "'''totalsentence is used to record the sum of all sentences'''\n",
    "totalsentence=1\n",
    "fileid=0\n",
    "\n",
    "fwrite = None\n",
    "for parent,dirnames,filenames in os.walk(read_path):\n",
    "    \n",
    "    listtemp=[]\n",
    "    for filename in filenames:\n",
    "        fileid=fileid+1\n",
    "        \n",
    "        '''embeding sentence_number for sentences in the same report '''\n",
    "        sentence_number=0\n",
    "        content=open(os.path.join(parent,filename),'r',encoding='utf-8')\n",
    "        \n",
    "        '''find the year information of annual report from title(consistent number)''' \n",
    "        year=re.findall(r\"\\d+\",filename)\n",
    "        \n",
    "        '''find the stock number of annual report from parent folder'''\n",
    "        stockid=parent[-6:]\n",
    "        \n",
    "        for line in content: \n",
    "            if re.match(pattern,line) or len(line)<8:\n",
    "                continue\n",
    "            elif re.search(\"。\",line):\n",
    "                sentences=line.split(\"。\")\n",
    "                for sentence in sentences:\n",
    "                    if len(sentence)<8 or sentence[-1]=='\\n':\n",
    "                        continue\n",
    "                    else:\n",
    "                        \n",
    "                        '''every 8000 statements in a csv file'''\n",
    "                        if totalsentence%8000==1:\n",
    "                            if fwrite != None:\n",
    "                                fwrite.close()\n",
    "                            fwrite = open(write_path+type_choose+\"_sentence\"+str(int(totalsentence/8000))+\".csv\",'a+',encoding='utf-8-sig',newline='')\n",
    "                        sentence=sentence+'。'\n",
    "                        sentence_number=sentence_number+1\n",
    "                        totalsentence=totalsentence+1\n",
    "                                                    \n",
    "                        '''stock_id, year， article id，sentence id, unlabelld tag, sentence content'''\n",
    "                        struct=[stockid,year[0],fileid,sentence_number,\"\",sentence]\n",
    "                        csv_write = csv.writer(fwrite,dialect='excel')\n",
    "                        csv_write.writerow(struct)\n",
    "                        #print(str(sentence_number)+\" \"+sentence)\n",
    "            \n",
    "                \n",
    "        '''some row reports are repeated, pick those by stockid and reporting year and record'''     \n",
    "        flag=0\n",
    "        struct_temp=[stockid,year[0]]\n",
    "        '''this report has been exist'''\n",
    "        if struct_temp in listtemp:\n",
    "            flag=1\n",
    "            \n",
    "        '''each time one report was completed,the statistics are record for further using'''    \n",
    "        listtemp.append(struct_temp)\n",
    "        \n",
    "        if type_choose=='mda':\n",
    "            with open(\"../../mid_data/number.csv\",'a+',encoding='utf-8-sig',newline='')as fstatistic:\n",
    "                '''writing item consists stockid  year  file_id sentence_sum，report path'''\n",
    "                statistic_struct=[stockid,year[0],fileid,sentence_number,parent+\"\\\\\"+filename]\n",
    "                if flag==1:\n",
    "                    statistic_struct=[\"Attention\",stockid,year[0],fileid,sentence_number,parent+\"\\\\\"+filename]\n",
    "\n",
    "                csv_write = csv.writer(fstatistic,dialect='excel')\n",
    "                csv_write.writerow(statistic_struct)\n",
    "             \n",
    "            \n",
    "fwrite.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third:  Cleaned  Repeated Sentences And Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     第二步把原始数据文件切成了一个个完整的待标记的句子，为了把处理好的数据交付人工标记，还需要删除重复数据并且乱序处理,这一步耗时较多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_path=\"../../mid_data/mda_clean2\"   \n",
    "write_path=\"../../mid_data/untagged_mda\"\n",
    "if type_choose==\"audit\":\n",
    "    read_path=\"../../mid_data/audit_clean2\"\n",
    "    write_path=\"../../mid_data/untagged_audit\"\n",
    "    \n",
    "'''every csv waitting to tagged contains 180 sentences'''\n",
    "csv_cap=180\n",
    "if not os.path.exists(write_path):\n",
    "    os.mkdir(write_path)\n",
    "sentences=[]     \n",
    "\n",
    "'''read in all items'''\n",
    "for parent,dirnames,filenames in os.walk(read_path):     \n",
    "    for file in filenames:\n",
    "        csv_file = csv.reader(open(os.path.join(read_path,file),'r',encoding='utf-8-sig')) \n",
    "           \n",
    "        sentencenum=0;\n",
    "        for sentence in csv_file:\n",
    "            sentences.append(sentence) \n",
    "            \n",
    "'''list1 contains only sentence content.''' \n",
    "'''list2 records the whole item, including stock number,year, and so on.'''\n",
    "'''for every item in list \"sentences\",if the content part has not been appeared in list1,''' \n",
    "'''then let the item join in list2,let the whole item join in list1'''\n",
    "list1=[]\n",
    "list2=[]\n",
    "s_num=0\n",
    "for s in sentences:\n",
    "    s_num +=1\n",
    "    print(\"\\r %d\" %(s_num),end=\"\")\n",
    "    if not s[5] in list1:\n",
    "        list1.append(s[5])\n",
    "        list2.append(s)\n",
    "print(\"\")\n",
    "print(\"清理前句子总数：\"+str(len(sentences)))\n",
    "print(\"清理后句子总数:\"+str(len(list2)))\n",
    "\n",
    "'''shuffle code'''\n",
    "random.shuffle(list2)\n",
    "\n",
    "fwrite = open(os.path.join(write_path,type_choose+\"sentence0.csv\"),'a+',encoding='utf-8-sig',newline='')\n",
    "item_number=0\n",
    "for item in list2:\n",
    "    if item_number%csv_cap==0:                            \n",
    "        fwrite.close()\n",
    "        fwrite = open(os.path.join(write_path,type_choose+\"sentence\"+str(filename[int(item_number/csv_cap)])+\".csv\"),'a+',encoding='utf-8-sig',newline='')\n",
    "    csv_write = csv.writer(fwrite,dialect='excel')\n",
    "    csv_write.writerow(item)\n",
    "    item_number=item_number+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
