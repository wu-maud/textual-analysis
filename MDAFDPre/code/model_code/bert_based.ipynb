{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../tmp', '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_experimental_distribute': None, '_evaluation_master': '', '_task_id': 0, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "  allow_growth: true\n",
      "}\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "    global_jit_level: ON_1\n",
      "  }\n",
      "}\n",
      ", '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D72EBAB748>, '_device_fn': None, '_protocol': None, '_train_distribute': None, '_task_type': 'worker', '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_log_step_count_steps': 100, '_eval_distribute': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_keep_checkpoint_max': 5, '_service': None}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: ../tmp, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "from extract_feature import BertVector\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.models import load_model\n",
    "\n",
    "bv = BertVector()\n",
    "hidden_size=768\n",
    "seq_len=128\n",
    "\n",
    "'''data_path defines where \"data.csv\" is(from extract_trainset.ipynb).'''\n",
    "data_path=\"../../mid_data/training_data/mda_data\"\n",
    "'''model_path defines where the trained model will save'''\n",
    "model_path=\"../../model/bert_model\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Training set, Test set\n",
    "####  根据extract_trainset.ipynb 中生成的data.csv，得到 train.csv,dev.csv,test.csv\n",
    "- 从原csv数据中抽取训练模型时需要的句子内容，具体标签。\n",
    "- 按照标签分为不同的list，-1为消极，0为中性，1为积极。统计各个标签的数据量。\n",
    "- 由于数据的不均衡，按照最少标签数据量向其他两个list中随机取相同数量的数据。\n",
    "- 按比例将标签均衡的数据集分为train set 和dev set。\n",
    "- 把剩下所有其他数据归入test set。\n",
    "- 返回三个csv 文件 后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_positive:18464 len_neutral:20097 len_negative:4504\n",
      "len_positive_test:898 len_neutral_test:890 len_negative_test:889\n",
      "10809\n",
      "2677\n",
      "29400\n"
     ]
    }
   ],
   "source": [
    "def build_trainset_testset(scale):\n",
    "    data_positive=list()\n",
    "    data_negative=list()\n",
    "    data_neutral=list()\n",
    "    data_all=list()\n",
    "    data_file=csv.reader(open(os.path.join(data_path,\"data.csv\"),'r',encoding='utf-8-sig'))\n",
    "    \n",
    "    \n",
    "    for line in data_file:\n",
    "        \n",
    "        if line[4]=='1':\n",
    "            data_positive.append([line[4],line[5]])\n",
    "            data_all.append([line[4],line[5]])\n",
    "        elif line[4]=='-1':\n",
    "            data_negative.append([line[4],line[5]])\n",
    "            data_all.append([line[4],line[5]])\n",
    "        elif line[4]=='0':\n",
    "            data_neutral.append([line[4],line[5]])\n",
    "            data_all.append([line[4],line[5]])\n",
    "        \n",
    "    print(\"len_positive:\"+str(len(data_positive))+\" len_neutral:\"+str(len(data_neutral))+\" len_negative:\"+str(len(data_negative)) )  \n",
    "    data_min_num=min([len(data_positive),len(data_neutral),len(data_negative)])\n",
    "    \n",
    "    data_positive = random.sample(data_positive, data_min_num)\n",
    "    data_negative =random.sample(data_negative, data_min_num)\n",
    "    data_neutral = random.sample(data_neutral,  data_min_num)\n",
    "    data_test=[item for item in data_all if item not in data_positive and item not in data_neutral and item not in data_negative]\n",
    "    \n",
    "    data_positive_train = random.sample(data_positive, int(scale*data_min_num))\n",
    "    data_negative_train = random.sample(data_negative,  int(scale*data_min_num))\n",
    "    data_neutral_train = random.sample(data_neutral,  int(scale*data_min_num))  \n",
    "    data_positive_test=[item for item in data_positive if item not in data_positive_train]\n",
    "    data_negative_test=[item for item in data_negative if item not in data_negative_train]\n",
    "    data_neutral_test=[item for item in data_neutral if item not in data_neutral_train]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"len_positive_test:\"+str(len(data_positive_test))+\" len_neutral_test:\"+str(len(data_neutral_test))+\" len_negative_test:\"+str(len(data_negative_test)) )\n",
    "    data_train=data_positive_train+data_negative_train+data_neutral_train\n",
    "    data_dev=data_positive_test+data_negative_test+data_neutral_test\n",
    "    print(len(data_train))\n",
    "    print(len(data_dev))\n",
    "    print(len(data_test))\n",
    "    f_train=open(os.path.join(data_path,\"train.csv\"),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "    for item in data_train:\n",
    "        f_train_write=csv.writer(f_train,dialect='excel')\n",
    "        f_train_write.writerow(item)\n",
    "    f_train.close()\n",
    "    \n",
    "    f_dev=open(os.path.join(data_path,\"dev.csv\"),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "    for item in data_dev:\n",
    "        f_dev_write=csv.writer(f_dev,dialect='excel')\n",
    "        f_dev_write.writerow(item)\n",
    "    f_dev.close()\n",
    "    \n",
    "    f_test=open(os.path.join(data_path,\"test.csv\"),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "    for item in data_test:\n",
    "        f_test_write=csv.writer(f_test,dialect='excel')\n",
    "        f_test_write.writerow(item)\n",
    "    f_test.close()\n",
    "\n",
    "\n",
    "'''the rate to depart total data into train set and dev set is 80% '''\n",
    "build_trainset_testset(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building train.npy,dev.npy,test.npy\n",
    "\n",
    "#### 从train.csv,dev.csv,test.csv 中读取句子和相应标签并构建数组，np.shape=（句子数，128，768）\n",
    "- bert 对文本进行embedding的过程比较缓慢，因此在转换完毕后，为了方便后续使用，会将转换好的numpy 保存。\n",
    "- 读入保存的numpy文件。每一条句子的embedding是句子长度(seq_len=128）*768（bert中文版每一个字的编码长度）\n",
    "- 机器学习用到的句子编码需要在bert编码基础上进行平均改动。机器学习每个句子的编码长度是1*768。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''transfer sentence to embedding'''\n",
    "def bert_rep_sentencevector(sentence):\n",
    "    sentence=sentence.replace(\" \",\"\")\n",
    "    embedding_matrix = np.array(bv.encode([sentence]))\n",
    "    return embedding_matrix\n",
    "    \n",
    "'''building array train,dev,test from train.csv, dev.csv, test.csv'''\n",
    "def build_traindata():\n",
    "    X_train = list()\n",
    "    Y_train = list()\n",
    "    X_test = list()\n",
    "    Y_test = list()\n",
    "    X_dev=list()\n",
    "    Y_dev=list()\n",
    " \n",
    "\n",
    "    account=0    \n",
    "    print(\"------start building train dataset------\")    \n",
    "    for line in csv.reader(open(os.path.join(data_path,\"train.csv\"),mode='r',encoding='utf-8-sig')):        \n",
    "        sent_vector = bert_rep_sentencevector(line[1])        \n",
    "        X_train.append(sent_vector)\n",
    "        \n",
    "        if line[0] == '1':\n",
    "            Y_train.append([0,0,1])\n",
    "        elif line[0]=='0':\n",
    "            Y_train.append([0,1,0])\n",
    "        elif line[0]=='-1':\n",
    "            Y_train.append([1,0,0])\n",
    "        else:\n",
    "            print(\"出错\")\n",
    "            print(line[0])   \n",
    "        account +=1\n",
    "        print(\"\\r %d\" %(account),end=\" \")       \n",
    "    print(\"\\n------end building train dataset------\")\n",
    "\n",
    "    \n",
    "    account=0\n",
    "    print(\"------start building dev dataset------\")\n",
    "    for line in csv.reader(open(os.path.join(data_path,\"dev.csv\"),mode='r',encoding='utf-8-sig')):        \n",
    "        sent_vector = bert_rep_sentencevector(line[1])\n",
    "        X_dev.append(sent_vector)\n",
    "        if line[0] == '1':\n",
    "            Y_dev.append([0,0,1])\n",
    "        elif line[0]=='0':\n",
    "            Y_dev.append([0,1,0])\n",
    "        elif line[0]=='-1':\n",
    "            Y_dev.append([1,0,0])\n",
    "        else:\n",
    "            print(\"出错\")\n",
    "            print(line[0])\n",
    "        account +=1\n",
    "        print(\"\\r %d\" %(account),end=\" \")\n",
    "    print(\"\\n------end building dev dataset------\")\n",
    "    \n",
    "    \n",
    "    account=0\n",
    "    print(\"------start building test dataset------\")\n",
    "    for line in csv.reader(open(os.path.join(data_path,\"test.csv\"),mode='r',encoding='utf-8-sig')):\n",
    "        \n",
    "        sent_vector = bert_rep_sentencevector(line[1])\n",
    "        X_test.append(sent_vector)\n",
    "        if line[0] == '1':\n",
    "            Y_test.append([0,0,1])\n",
    "        elif line[0]=='0':\n",
    "            Y_test.append([0,1,0])\n",
    "        elif line[0]=='-1':\n",
    "            Y_test.append([1,0,0])\n",
    "        else:\n",
    "            print(\"出错\")\n",
    "            print(line[0])\n",
    "        account +=1\n",
    "        print(\"\\r %d\" %(account),end=\" \")\n",
    "    print(\"\\n------end building test dataset------\")\n",
    " \n",
    "\n",
    "    return np.array(X_train), np.array(Y_train), np.array(X_dev), np.array(Y_dev),np.array(X_test), np.array(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2, Y_train_2, X_dev_2, Y_dev_2 ,X_test_2,Y_test_2= build_traindata()\n",
    "\n",
    "X_train_2=np.squeeze(X_train_2)\n",
    "X_dev_2=np.squeeze(X_dev_2)\n",
    "X_test_2=np.squeeze(X_test_2)\n",
    "\n",
    "np.save(data_path+\"/trainX_vec_2\",X_train_2)\n",
    "np.save(data_path+\"/devX_vec_2\",X_dev_2) \n",
    "np.save(data_path+\"/testX_vec_2\",X_test_2)      \n",
    "np.save(data_path+\"/trainY_vec_2\",Y_train_2)\n",
    "np.save(data_path+\"/devY_vec_2\",Y_dev_2)\n",
    "np.save(data_path+\"/testY_vec_2\",Y_test_2)\n",
    "\n",
    "\n",
    "print(X_train_2.shape, Y_train_2.shape)\n",
    "print(X_dev_2.shape, Y_dev_2.shape)\n",
    "print(X_test_2.shape, Y_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4113, 128, 768) (4113, 3)\n",
      "(1013, 128, 768) (1013, 3)\n",
      "(9074, 128, 768) (9074, 3)\n",
      "(4113, 768) (4113,)\n",
      "(1013, 768) (1013,)\n",
      "(9074, 768) (9074,)\n"
     ]
    }
   ],
   "source": [
    "def Label_Judge(Y):\n",
    "    assert np.sum(Y)==1\n",
    "    if Y[0]==1:\n",
    "        return -1\n",
    "    elif Y[1]==1:\n",
    "        return 0\n",
    "    elif Y[2]==1:\n",
    "        return 1 \n",
    "       \n",
    "X_train_2=np.load(data_path+\"/trainX_vec_2.npy\")\n",
    "X_dev_2=np.load(data_path+\"/devX_vec_2.npy\")\n",
    "X_test_2=np.load(data_path+\"/testX_vec_2.npy\")\n",
    "Y_train_2=np.load(data_path+\"/trainY_vec_2.npy\")\n",
    "Y_dev_2=np.load(data_path+\"/devY_vec_2.npy\")\n",
    "Y_test_2=np.load(data_path+\"/testY_vec_2.npy\")\n",
    "\n",
    "'''average data to build the other numpies for machine learning '''\n",
    "X_train=X_train_2.mean(axis=1)\n",
    "X_dev=X_dev_2.mean(axis=1)\n",
    "X_test=X_test_2.mean(axis=1)\n",
    "\n",
    "\n",
    "Y_train=np.zeros(Y_train_2.shape[0])\n",
    "for i in range(Y_train_2.shape[0]):\n",
    "    Y_train[i]=Label_Judge(Y_train_2[i]) \n",
    "    \n",
    "Y_dev=np.zeros(Y_dev_2.shape[0])\n",
    "for i in range(Y_dev_2.shape[0]):\n",
    "    Y_dev[i]=Label_Judge(Y_dev_2[i])\n",
    "    \n",
    "Y_test=np.zeros(Y_test_2.shape[0])\n",
    "for i in range(Y_test_2.shape[0]):\n",
    "    Y_test[i]=Label_Judge(Y_test_2[i])\n",
    "\n",
    "print(X_train_2.shape,Y_train_2.shape)\n",
    "print(X_dev_2.shape, Y_dev_2.shape)\n",
    "print(X_test_2.shape, Y_test_2.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''default setting'''\n",
    "def train_svm(X_train, Y_train):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='linear',probability=True)\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,'sentiment_svm_model.m'))\n",
    "\n",
    "def evaluate_svm(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1))[0])\n",
    "    for index in range(len(Y_predict)):\n",
    "        if int(Y_predict[index]) == int(Y_test[index]):\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score)) #0.8302767589196399  model accuray is :0.77675891963988\n",
    "    return score\n",
    "\n",
    "\n",
    "def predict_svm(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(bert_rep_sentencevector(sentence1)).reshape(1,128,768).mean(axis=1)\n",
    "    rep_sen2 = np.array(bert_rep_sentencevector(sentence2)).reshape(1,128,768).mean(axis=1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1)) #sentence1 [1]\n",
    "    print('sentence2', model.predict_proba(rep_sen2)) #sentence2 [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4113, 768) (4113,)\n",
      "(1013, 768) (1013,)\n",
      "(9074, 768) (9074,)\n",
      "model accuray is :0.8262586377097729\n",
      "model accuray is :0.7956799647344059\n",
      "sentence1 [[1.78321454e-04 1.53821599e-02 9.84439519e-01]]\n",
      "sentence2 [[9.97290486e-01 2.22656378e-03 4.82950567e-04]]\n"
     ]
    }
   ],
   "source": [
    "train_svm(X_train, Y_train)\n",
    "model_filepath_svm = os.path.join(model_path,'sentiment_svm_model.m')\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "evaluate_svm(model_filepath_svm, X_dev, Y_dev)\n",
    "evaluate_svm(model_filepath_svm, X_test, Y_test)\n",
    "predict_svm(model_filepath_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_bayes(X_train, Y_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,'sentiment_bayes_model.m'))\n",
    "\n",
    "\n",
    "def evaluate_bayes(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1))[0])\n",
    "    for index in range(len(Y_predict)):\n",
    "        if int(Y_predict[index]) == int(Y_test[index]):\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score))\n",
    "    return score\n",
    "\n",
    "\n",
    "def predict_bayes(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(bert_rep_sentencevector(sentence1)).reshape(1,128,768).mean(axis=1)\n",
    "    rep_sen2 = np.array(bert_rep_sentencevector(sentence2)).reshape(1,128,768).mean(axis=1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1))\n",
    "    print('sentence2', model.predict_proba(rep_sen2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29434, 768) (29434,)\n",
      "model accuray is :0.666270299653462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.666270299653462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filepath_bayes = os.path.join(model_path,'sentiment_bayes_model.m')\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "train_bayes(X_train, Y_train)\n",
    "evaluate_bayes(model_filepath_bayes, X_dev, Y_dev)\n",
    "evaluate_bayes(model_filepath_bayes, X_test, Y_test)\n",
    "predict_bayes(model_filepath_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_knn(X_train, Y_train, X_test, Y_test):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    for x in range(1,101,5):\n",
    "        model = KNeighborsClassifier(n_neighbors=x)\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        num = 0\n",
    "        num = 0\n",
    "        preds = preds.tolist()\n",
    "        for i, pred in enumerate(preds):\n",
    "            if int(pred) == int(Y_test[i]):\n",
    "                num += 1\n",
    "        print('K= ' + str(x) + ', precision_score:' + str(float(num) / len(preds)))\n",
    " \n",
    "    '''choose k=14 to train and build model'''\n",
    "    model = KNeighborsClassifier(n_neighbors=14)\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,'sentiment_knn_model.m'))\n",
    "\n",
    "    \n",
    "def evaluate_knn(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1)))\n",
    "    for index in range(len(Y_predict)):\n",
    "        if Y_predict[index] == Y_test[index]:\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score))#0.7909303101033678\n",
    "    return score\n",
    "\n",
    "\n",
    "def predict_knn(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(bert_rep_sentencevector(sentence1)).reshape(1,128,768).mean(axis=1)\n",
    "    rep_sen2 = np.array(bert_rep_sentencevector(sentence2)).reshape(1,128,768).mean(axis=1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1))\n",
    "    print('sentence2', model.predict_proba(rep_sen2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29434, 768) (29434,)\n",
      "model accuray is :0.6992593599238975\n",
      "sentence1 [[0.05882353 0.05882353 0.88235294]]\n",
      "sentence2 [[0.58823529 0.17647059 0.23529412]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath_knn = os.path.join(model_path,'sentiment_knn_model.m')\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "train_knn(X_train, Y_train, X_dev, Y_dev)\n",
    "evaluate_knn(model_filepath_knn, X_dev, Y_dev)\n",
    "evaluate_knn(model_filepath_knn, X_test, Y_test)\n",
    "predict_knn(model_filepath_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decsion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_decisiontree(X_train, Y_train):\n",
    "    from sklearn import tree\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,'sentiment_decisiontree_model.m'))\n",
    "\n",
    "def evaluate_decisiontree(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1))[0])\n",
    "    for index in range(len(Y_predict)):\n",
    "        if int(Y_predict[index]) == int(Y_test[index]):\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score)) #0.6907302434144715\n",
    "    return score\n",
    "\n",
    "def predict_decisiontree(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(bert_rep_sentencevector(sentence1)).reshape(1,128,768).mean(axis=1)\n",
    "    rep_sen2 = np.array(bert_rep_sentencevector(sentence2)).reshape(1,128,768).mean(axis=1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1)) #sentence1 [0]\n",
    "    print('sentence2', model.predict_proba(rep_sen2)) #sentence2 [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29434, 768) (29434,)\n",
      "model accuray is :0.6105524223686892\n",
      "sentence1 [[0. 0. 1.]]\n",
      "sentence2 [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath_tree = os.path.join(model_path,'sentiment_decisiontree_model.m')\n",
    "train_decisiontree(X_train, Y_train)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "evaluate_decisiontree(model_filepath_tree, X_dev, Y_dev)\n",
    "evaluate_decisiontree(model_filepath_tree, X_test, Y_test)\n",
    "predict_decisiontree(model_filepath_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''six layers CNN'''\n",
    "def train_cnn(X_train, Y_train, X_test, Y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "    \n",
    "    '''build sequential model'''\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, 3, activation='relu', input_shape=(seq_len, hidden_size)))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    history=model.fit(X_train, Y_train, batch_size=100, epochs=5,shuffle=True,validation_data=(X_test, Y_test))\n",
    "    model.save(os.path.join(model_path,'sentiment_cnn_model.h5'))\n",
    "   \n",
    "    return history\n",
    "\n",
    "def evaluate_cnn(X_test,Y_test,model_filepath):\n",
    "    model=load_model(model_filepath)\n",
    "    loss,accuracy = model.evaluate(X_test,Y_test)\n",
    "    print('model accuracy is :{0}'.format(accuracy))\n",
    "    \n",
    "def predict_cnn(model_filepath): \n",
    "    model = load_model(model_filepath)\n",
    "    sentence = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'  \n",
    "    sentence_vector = np.squeeze(np.array([bert_rep_sentencevector(sentence)]),axis=1) \n",
    "    print('test after load: ', model.predict(sentence_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4113, 128, 768) (4113, 3)\n",
      "(1013, 128, 768) (1013, 3)\n",
      "(9074, 128, 768) (9074, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 128)          295040    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 39, 64)            24640     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 37, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 10, 32)            6176      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 8, 32)             3104      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 390,691\n",
      "Trainable params: 390,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4113 samples, validate on 1013 samples\n",
      "Epoch 1/5\n",
      "4113/4113 [==============================] - 24s 6ms/step - loss: 0.5698 - acc: 0.6864 - val_loss: 0.4641 - val_acc: 0.8253\n",
      "Epoch 2/5\n",
      "4113/4113 [==============================] - 22s 5ms/step - loss: 0.3827 - acc: 0.8326 - val_loss: 0.3384 - val_acc: 0.8773\n",
      "Epoch 3/5\n",
      "4113/4113 [==============================] - 22s 5ms/step - loss: 0.3234 - acc: 0.8670 - val_loss: 0.3730 - val_acc: 0.8598\n",
      "Epoch 4/5\n",
      "4113/4113 [==============================] - 22s 5ms/step - loss: 0.2930 - acc: 0.8866 - val_loss: 0.2735 - val_acc: 0.8894\n",
      "Epoch 5/5\n",
      "4113/4113 [==============================] - 22s 5ms/step - loss: 0.2629 - acc: 0.8959 - val_loss: 0.3329 - val_acc: 0.8865\n",
      "9074/9074 [==============================] - 21s 2ms/step\n",
      "model accuracy is :0.8697377185688057\n",
      "test after load:  [[2.9997061e-08 5.6638786e-05 9.9994290e-01]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath = os.path.join(model_path,'sentiment_cnn_model.h5')\n",
    "print(X_train_2.shape, Y_train_2.shape)\n",
    "print(X_dev_2.shape, Y_dev_2.shape)\n",
    "print(X_test_2.shape, Y_test_2.shape)\n",
    "train_cnn(X_train_2, Y_train_2, X_dev_2, Y_dev_2)\n",
    "evaluate_cnn(X_test_2,Y_test_2,model_filepath)\n",
    "predict_cnn(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''4 layers of LSTM'''\n",
    "def train_lstm(X_train, Y_train, X_test, Y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,Bidirectional\n",
    "    import numpy as np\n",
    "    data_dim = hidden_size   \n",
    "    timesteps = seq_len  \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True,\n",
    "                   input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 64\n",
    "    model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(32))# return a single vector of dimension 32\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history=model.fit(X_train, Y_train, batch_size=100, epochs=7,shuffle=True, validation_data=(X_test, Y_test))\n",
    "    model.summary()\n",
    "    model.save(os.path.join(model_path,'sentiment_lstm_model.h5'))\n",
    "\n",
    "    return history\n",
    "def evaluate_lstm(X_test,Y_test,model_filepath):\n",
    "    model=load_model(model_filepath)\n",
    "    loss,accuracy = model.evaluate(X_test,Y_test)\n",
    "    print('model accuracy is :{0}'.format(accuracy))\n",
    "    \n",
    "\n",
    "def predict_lstm(model_filepath):\n",
    "    model = load_model(model_filepath)\n",
    "    sentence = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。\"   \n",
    "    sentence_vector = np.squeeze(np.array([bert_rep_sentencevector(sentence)]),axis=1)\n",
    "    print('test after load: ', model.predict(sentence_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10809, 128, 768) (10809, 3)\n",
      "(2687, 128, 768) (2687, 3)\n",
      "Train on 10809 samples, validate on 2687 samples\n",
      "Epoch 1/7\n",
      "10809/10809 [==============================] - 99s 9ms/step - loss: 0.7469 - acc: 0.6741 - val_loss: 0.6282 - val_acc: 0.7685\n",
      "Epoch 2/7\n",
      "10809/10809 [==============================] - 98s 9ms/step - loss: 0.5191 - acc: 0.7999 - val_loss: 0.5129 - val_acc: 0.7923\n",
      "Epoch 3/7\n",
      "10809/10809 [==============================] - 97s 9ms/step - loss: 0.4534 - acc: 0.8252 - val_loss: 0.4754 - val_acc: 0.8210\n",
      "Epoch 4/7\n",
      "10809/10809 [==============================] - 99s 9ms/step - loss: 0.4145 - acc: 0.8448 - val_loss: 0.4202 - val_acc: 0.8433\n",
      "Epoch 5/7\n",
      "10809/10809 [==============================] - 101s 9ms/step - loss: 0.3935 - acc: 0.8456 - val_loss: 0.4174 - val_acc: 0.8389\n",
      "Epoch 6/7\n",
      "10809/10809 [==============================] - 101s 9ms/step - loss: 0.3653 - acc: 0.8602 - val_loss: 0.4940 - val_acc: 0.8188\n",
      "Epoch 7/7\n",
      "10809/10809 [==============================] - 103s 10ms/step - loss: 0.3488 - acc: 0.8656 - val_loss: 0.4409 - val_acc: 0.8515\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 128, 64)           213248    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 128, 32)           12416     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 128, 32)           8320      \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 242,403\n",
      "Trainable params: 242,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "test after load:  [[0.00328988 0.03353607 0.963174  ]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath_lstm = os.path.join(model_path,'sentiment_lstm_model.h5')\n",
    "print(X_train_2.shape, Y_train_2.shape)\n",
    "print(X_dev_2.shape, Y_dev_2.shape)\n",
    "print(X_test_2.shape, Y_test_2.shape)\n",
    "history_lstm=train_lstm(X_train_2, Y_train_2, X_dev_2, Y_dev_2)\n",
    "evaluate_lstm(X_test_2,Y_test_2,model_filepath_lstm)\n",
    "predict_lstm(model_filepath_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
