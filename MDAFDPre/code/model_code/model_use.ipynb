{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Use\n",
    "#### 用训练好的模型去评估每篇文章的每一个句子，得到每一个句子的积极程度评分，最后取平均得到每篇年报mda部分与audit部分的积极程度分数。\n",
    "- mda_model_filepath:计算 mda部分句子的分数所用的模型路径。\n",
    "- audit_model_filepath:计算audit部分句子分数所用的模型路径。\n",
    "- dict_file_path: data_clean.ipynb 生成的每篇年报的统计信息。\n",
    "- dic_write_path: 得到每篇文章mda部分和audit部分的分数后，接在dict_file_path文件后写入dic_write_path。\n",
    "- mda_data_path：data_clean.ipynb 中，生成的每篇年报所有mda句子信息的所在文件。\n",
    "- mda_data_path：写入分数后的mda句子文件写出地址\n",
    "- audit_data_path：data_clean.ipynb 中，生成的每篇年报所有audit句子信息的所在文件。\n",
    "- audit_write_path：写入分数后的audit句子文件写出地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_device_fn': None, '_save_checkpoints_steps': None, '_model_dir': '../tmp', '_master': '', '_keep_checkpoint_max': 5, '_protocol': None, '_evaluation_master': '', '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "  allow_growth: true\n",
      "}\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "    global_jit_level: ON_1\n",
      "  }\n",
      "}\n",
      ", '_save_summary_steps': 100, '_num_worker_replicas': 1, '_service': None, '_task_id': 0, '_is_chief': True, '_global_id_in_cluster': 0, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001749A07C1D0>, '_log_step_count_steps': 100, '_task_type': 'worker', '_experimental_distribute': None, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_train_distribute': None, '_eval_distribute': None, '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: ../tmp, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import gensim\n",
    "import random\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import csv\n",
    "import matplotlib as mpt\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from extract_feature import BertVector\n",
    "mda_model_filepath=\"../../model/bert_model/sentiment_cnn_model.h5\"\n",
    "audit_model_filepath=\"../../model/word2vec_model/sentiment_cnn_model.h5\"\n",
    "dict_filepath=\"../../mid_data/number.csv\"\n",
    "dict_writepath=\"../../score/score.csv\"\n",
    "mda_data_path=\"../../mid_data/mda_clean2\"\n",
    "mda_write_path=\"../../score/mda_score\"\n",
    "audit_data_path=\"../../mid_data/audit_clean2\"\n",
    "audit_write_path=\"../../score/audit_score\"\n",
    "bv = BertVector()\n",
    "embedding_dim=250\n",
    "seq_len=128\n",
    "if_deep=True\n",
    "\n",
    "bert_model = load_model(mda_model_filepath)\n",
    "audit_model= load_model(audit_model_filepath)\n",
    "\n",
    "if not os.path.exists(mda_write_path):\n",
    "    os.makedirs(mda_write_path)\n",
    "if not os.path.exists(audit_write_path):  \n",
    "    os.makedirs(audit_write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "'''lead ino pre-parpared word vector model file''' \n",
    "VECTOR_DIR = './embedding/word_vector.bin'  # 词向量模型文件\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(VECTOR_DIR, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立字典\n",
    "#### 所有文章的统计信息用dataframe表示，增加五列。\n",
    "- 一列index由stock_id 和year构成，方便后续定位；\n",
    "- 一列mda_sentence 用来mda 句子的计数，一列mda_score 用来统计mda句子的分数和；\n",
    "- 一列audit_sentence，一列 audit_score 和mda同理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv2dict(file_path):\n",
    "    \n",
    "    new_dataframe = pd.read_csv(file_path, header=None,names=[\"stock_id\",\"year\",\"mda_id\",\"mda_sentence_num\",\"file_path\"],dtype={'stock_id':str,'year':str,'mda_id':str,'mda_sentence_num':np.int32,'file_path':str})\n",
    "    new_dataframe[\"mda_sentence\"]=[0]*new_dataframe.mda_id.count()\n",
    "    new_dataframe[\"audit_sentence\"]=[0]*new_dataframe.mda_id.count()\n",
    "    new_dataframe[\"mda_score\"]=[0.0]*new_dataframe.mda_id.count()\n",
    "    new_dataframe[\"audit_score\"]=[0.0]*new_dataframe.mda_id.count()\n",
    "    \n",
    "    new_dataframe[\"index\"]=new_dataframe.stock_id+\"/\"+new_dataframe.year\n",
    "    new_dataframe.set_index([\"index\"], inplace=True)\n",
    "    return new_dataframe\n",
    "\n",
    "def dict2csv(diction,file):\n",
    "    diction.to_csv(file)\n",
    "\n",
    "''' bert_based embedding'''\n",
    "def bert_rep_sentencevector_2(sentence):\n",
    "    sentence=sentence.replace(\" \",\"\")\n",
    "    embedding_matrix = np.array(bv.encode([sentence]))\n",
    "    return embedding_matrix \n",
    "\n",
    "'''word2vec_based embedding'''\n",
    "def rep_sentencevector(sentence,if_deep=True):\n",
    "    \n",
    "    '''participle''' \n",
    "    word_list = jieba.lcut(sentence, cut_all=True)\n",
    "    \n",
    "    while '' in word_list:\n",
    "        word_list.remove('')\n",
    "    embedding_dim = 250\n",
    "    if not if_deep:\n",
    "        embedding_matrix = np.zeros(embedding_dim)\n",
    "        for index, word in enumerate(word_list):\n",
    "            try:\n",
    "                embedding_matrix += model[word]\n",
    "            except:\n",
    "                pass\n",
    "        return embedding_matrix/len(word_list)\n",
    "    else:\n",
    "        max_words=seq_len\n",
    "        embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "        for index, word in enumerate(word_list):\n",
    "            try:\n",
    "                embedding_matrix[index] = model[word]\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "'''get a positive index from an array of probabilities'''\n",
    "def compute_score(score_list):    \n",
    "    return -1*score_list[0]+1*score_list[2]\n",
    "\n",
    "'''mda model predict mda sentence'''\n",
    "def predict_mda(sentence_vector,if_deep=True):\n",
    "\n",
    "    if if_deep==False:\n",
    "        sentence_vector=sentence_vector.mean(axis=1)\n",
    "    score_list=np.squeeze(bert_model.predict(sentence_vector))\n",
    "    \n",
    "    return compute_score(score_list)\n",
    "\n",
    "'''audit model predict audit sentence'''\n",
    "def predict_audit(sentence,if_deep=True):    \n",
    "    sentence_vector = np.array([rep_sentencevector(sentence,if_deep)])\n",
    "    score_list=np.squeeze(audit_model.predict(sentence_vector))   \n",
    "    return compute_score(score_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_list=csv2dict(dict_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算mda_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for parent,dirnames,filenames in os.walk(mda_data_path):  \n",
    "    for file in filenames:    \n",
    "        csv_reader=csv.reader(open(os.path.join(mda_data_path,file),'r',encoding='utf-8-sig'))\n",
    "        csv_writer_open=open(os.path.join(mda_write_path,file),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "        csv_writer=csv.writer(csv_writer_open,dialect='excel')\n",
    "        for item in csv_reader:\n",
    "            '''use stock_id and year information to combine index'''\n",
    "            item[0]=item[0].rjust(6,'0')\n",
    "            key=item[0]+\"/\"+item[1]\n",
    "            assert len(key)==11\n",
    "            \n",
    "            if key in list(score_list.index):                \n",
    "                               \n",
    "                array=np.squeeze(np.array([bert_rep_sentencevector_2(item[5])]),axis=0)\n",
    "               \n",
    "                score=predict_mda(array,if_deep)\n",
    "                score_list.loc[key,\"mda_score\"]+=score\n",
    "                score_list.loc[key,\"mda_sentence\"]+=1\n",
    "                count+=1\n",
    "                print(\"\\r %d, %s:%f                                                                                                               \"% (count,item[5],score),end=\" \")\n",
    "                item[4]=score\n",
    "                csv_writer.writerow(item)\n",
    "        csv_writer_open.close() \n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算audit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for parent,dirnames,filenames in os.walk(audit_data_path):  \n",
    "    for file in filenames:\n",
    "        csv_reader=csv.reader(open(os.path.join(audit_data_path,file),'r',encoding='utf-8-sig'))\n",
    "        csv_writer_open=open(os.path.join(audit_write_path,file),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "        csv_writer=csv.writer(csv_writer_open,dialect='excel')\n",
    "        for item in csv_reader:\n",
    "            item[0]=item[0].rjust(6,'0')\n",
    "            key=item[0]+\"/\"+item[1]\n",
    "            assert len(key)==11\n",
    "            if key in list(score_list.index):\n",
    "                score=predict_audit(item[5],if_deep)\n",
    "                score_list.loc[key,\"audit_score\"]+=score\n",
    "                score_list.loc[key,\"audit_sentence\"]+=1\n",
    "                \n",
    "                count+=1\n",
    "                print(\"\\r %d, %s %s:%f                                                                                                               \"% (count,key,item[5],score),end=\" \")\n",
    "                item[4]=score  \n",
    "                csv_writer.writerow(item)\n",
    "        csv_writer_open.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score List 算分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "score_list=csv2dict(dict_filepath)\n",
    "count=0\n",
    "for parent,dirnames,filenames in os.walk(mda_write_path): \n",
    "    for file in filenames:    \n",
    "        print(file)\n",
    "        csv_reader=csv.reader(open(os.path.join(mda_write_path,file),'r',encoding='utf-8-sig'))\n",
    "        for item in csv_reader:\n",
    "            item[0]=item[0].rjust(6,'0')\n",
    "            key=item[0]+\"/\"+item[1]\n",
    "            assert len(key)==11           \n",
    "            if key in list(score_list.index):                \n",
    "                                \n",
    "                score=predict_mda(array,if_deep)\n",
    "                score_list.loc[key,\"mda_score\"]+=float(item[4])\n",
    "                score_list.loc[key,\"mda_sentence\"]+=1\n",
    "                count+=1\n",
    "                print(\"\\r %d, %s:%f                                                                                                               \"% (count,item[5],float(item[4])),end=\" \")\n",
    "count=0                \n",
    "for parent,dirnames,filenames in os.walk(audit_write_path): \n",
    "    for file in filenames:    \n",
    "        print(file)\n",
    "        csv_reader=csv.reader(open(os.path.join(audit_write_path,file),'r',encoding='utf-8-sig'))\n",
    "        for item in csv_reader:\n",
    "            item[0]=item[0].rjust(6,'0')\n",
    "            key=item[0]+\"/\"+item[1]\n",
    "            assert len(key)==11\n",
    "            \n",
    "            if key in list(score_list.index):                \n",
    "                \n",
    "                \n",
    "                score=predict_mda(array,if_deep)\n",
    "                score_list.loc[key,\"audit_score\"]+=float(item[4])\n",
    "                score_list.loc[key,\"audit_sentence\"]+=1\n",
    "                count+=1\n",
    "                print(\"\\r %d, %s:%f                                                                                                               \"% (count,item[5],float(item[4])),end=\" \")               \n",
    "\n",
    "score_list[\"mda_score\"]=score_list[\"mda_score\"]/score_list[\"mda_sentence\"]\n",
    "score_list[\"audit_score\"]=score_list[\"audit_score\"]/score_list[\"audit_sentence\"]\n",
    "dict2csv(score_list,dict_writepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
