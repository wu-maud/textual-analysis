{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import gensim\n",
    "import random\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import csv\n",
    "import matplotlib as mpt\n",
    "import jieba\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "hidden_size=768\n",
    "seq_len=128\n",
    "\n",
    "'''data_path defines where \"data.csv\" is(from extract_trainset.ipynb).'''\n",
    "data_path=\"../../mid_data/training_data/mda_data\"\n",
    "'''model_path defines where the trained model will save'''\n",
    "model_path=\"../../model/word2vec_model\"\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''lead ino pre-parpared word vector model file'''\n",
    "VECTOR_DIR = './dictionary/word_vector.bin'  \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(VECTOR_DIR, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building Training set, Test set\n",
    "####  根据extract_trainset.ipynb 中生成的data.csv，得到 train.csv,dev.csv,test.csv(如果已经存在此类的csv，可以跳过)\n",
    "- 从原csv数据中抽取训练模型时需要的句子内容，具体标签。\n",
    "- 按照标签分为不同的list，-1为消极，0为中性，1为积极。统计各个标签的数据量。\n",
    "- 由于数据的不均衡，按照最少标签数据量向其他两个list中随机取相同数量的数据。\n",
    "- 按比例将标签均衡的数据集分为train set 和dev set。\n",
    "- 把剩下所有其他数据归入test set。\n",
    "- 返回三个csv 文件 后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_positive:18464 len_neutral:20097 len_negative:4504\n",
      "len_positive_test:899 len_neutral_test:891 len_negative_test:889\n",
      "10809\n",
      "2679\n",
      "29406\n"
     ]
    }
   ],
   "source": [
    "def build_trainset_testset(scale):\n",
    "    data_positive=list()\n",
    "    data_negative=list()\n",
    "    data_neutral=list()\n",
    "    data_all=list()\n",
    "    data_file=csv.reader(open(os.path.join(data_path,\"data.csv\"),'r',encoding='utf-8-sig'))\n",
    "    \n",
    "    \n",
    "    for line in data_file:\n",
    "        \n",
    "        if line[4]=='1':\n",
    "            data_positive.append([line[4],line[5]])\n",
    "            data_all.append([line[4],line[5]])\n",
    "        elif line[4]=='-1':\n",
    "            data_negative.append([line[4],line[5]])\n",
    "            data_all.append([line[4],line[5]])\n",
    "        elif line[4]=='0':\n",
    "            data_neutral.append([line[4],line[5]])\n",
    "            data_all.append([line[4],line[5]])\n",
    "        \n",
    "    print(\"len_positive:\"+str(len(data_positive))+\" len_neutral:\"+str(len(data_neutral))+\" len_negative:\"+str(len(data_negative)) )  \n",
    "    data_min_num=min([len(data_positive),len(data_neutral),len(data_negative)])\n",
    "    \n",
    "    data_positive = random.sample(data_positive, data_min_num)\n",
    "    data_negative =random.sample(data_negative, data_min_num)\n",
    "    data_neutral = random.sample(data_neutral,  data_min_num)\n",
    "    data_test=[item for item in data_all if item not in data_positive and item not in data_neutral and item not in data_negative]\n",
    "    \n",
    "    data_positive_train = random.sample(data_positive, int(scale*data_min_num))\n",
    "    data_negative_train = random.sample(data_negative,  int(scale*data_min_num))\n",
    "    data_neutral_train = random.sample(data_neutral,  int(scale*data_min_num))  \n",
    "    data_positive_test=[item for item in data_positive if item not in data_positive_train]\n",
    "    data_negative_test=[item for item in data_negative if item not in data_negative_train]\n",
    "    data_neutral_test=[item for item in data_neutral if item not in data_neutral_train]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"len_positive_test:\"+str(len(data_positive_test))+\" len_neutral_test:\"+str(len(data_neutral_test))+\" len_negative_test:\"+str(len(data_negative_test)) )\n",
    "    data_train=data_positive_train+data_negative_train+data_neutral_train\n",
    "    data_dev=data_positive_test+data_negative_test+data_neutral_test\n",
    "    print(len(data_train))\n",
    "    print(len(data_dev))\n",
    "    print(len(data_test))\n",
    "    f_train=open(os.path.join(data_path,\"train.csv\"),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "    for item in data_train:\n",
    "        f_train_write=csv.writer(f_train,dialect='excel')\n",
    "        f_train_write.writerow(item)\n",
    "    f_train.close()\n",
    "    \n",
    "    f_dev=open(os.path.join(data_path,\"dev.csv\"),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "    for item in data_dev:\n",
    "        f_dev_write=csv.writer(f_dev,dialect='excel')\n",
    "        f_dev_write.writerow(item)\n",
    "    f_dev.close()\n",
    "    \n",
    "    f_test=open(os.path.join(data_path,\"test.csv\"),mode=\"w\",encoding=\"utf-8-sig\",newline=\"\")\n",
    "    for item in data_test:\n",
    "        f_test_write=csv.writer(f_test,dialect='excel')\n",
    "        f_test_write.writerow(item)\n",
    "    f_test.close()\n",
    "\n",
    "\n",
    "'''the rate to depart total data into train set and dev set is 80% '''\n",
    "build_trainset_testset(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building array\n",
    "#### 从train.csv,dev.csv,test.csv 中读取句子和相应标签并构建数组。由于word2vec比较快，不需要保存向量，可以随时建立数据随时训练。\n",
    "- 对读入csv的每个句子，用jieba进行分词，并删除里面的空格，对每个分好的词语在词向量模型中找到相应向量。\n",
    "- 如果是为了训练深度学习的模型，每一个句子对应的向量是（1，128，250）。128是一个句子最长的分词数量，250是每个词语在词向量文件中对应的编码长度。\n",
    "- 如果是为了训练机器学习的模型，每一个句子对应的向量是（1,250），具体做法在上一步的基础上对向量进行平均处理即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.675 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rep_sentencevector(sentence,if_deep=False):\n",
    "    '''participle'''   \n",
    "    word_list = jieba.lcut(sentence, cut_all=True)\n",
    "    while '' in word_list:\n",
    "        word_list.remove('')\n",
    "    embedding_dim = 250\n",
    "    if not if_deep:\n",
    "        embedding_matrix = np.zeros(embedding_dim)\n",
    "        for index, word in enumerate(word_list):\n",
    "            try:\n",
    "                embedding_matrix += model[word]\n",
    "            except:\n",
    "                pass\n",
    "        return embedding_matrix/len(word_list)\n",
    "    else:\n",
    "        max_words=seq_len\n",
    "        embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "        for index, word in enumerate(word_list):\n",
    "            try:\n",
    "                embedding_matrix[index] = model[word]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "   \n",
    "\n",
    "def build_traindata(if_deep=False):\n",
    "    X_train = list()\n",
    "    Y_train = list()\n",
    "    X_dev = list()\n",
    "    Y_dev = list()\n",
    "    X_test = list()\n",
    "    Y_test = list()\n",
    "    data_path_list=[data_path]\n",
    "    for datapath in data_path_list:\n",
    "        for line in csv.reader(open(os.path.join(datapath,\"train.csv\"),mode='r',encoding='utf-8-sig')):\n",
    "\n",
    "            sent_vector = rep_sentencevector(line[1],if_deep)\n",
    "\n",
    "            X_train.append(sent_vector)\n",
    "            if line[0] == '1':\n",
    "                if if_deep==False:\n",
    "                    Y_train.append(1)\n",
    "                else:\n",
    "                    Y_train.append([0,0,1])\n",
    "            elif line[0]=='0':\n",
    "                if if_deep==False:\n",
    "                    Y_train.append(0)\n",
    "                else:\n",
    "                    Y_train.append([0,1,0])\n",
    "            elif line[0]=='-1':\n",
    "                if if_deep==False:\n",
    "                    Y_train.append(-1)\n",
    "                else:\n",
    "                    Y_train.append([1,0,0])\n",
    "            else:\n",
    "                print(\"wrong!\")\n",
    "\n",
    "        for line in csv.reader(open(os.path.join(datapath,\"dev.csv\"),mode='r',encoding='utf-8-sig')):\n",
    "\n",
    "            sent_vector = rep_sentencevector(line[-1],if_deep)\n",
    "            X_dev.append(sent_vector)\n",
    "            if line[0] == '1':\n",
    "                if if_deep==False:\n",
    "                    Y_dev.append(1)\n",
    "                else:\n",
    "                    Y_dev.append([0,0,1])\n",
    "            elif line[0]=='0':\n",
    "                if if_deep==False:\n",
    "                    Y_dev.append(0)\n",
    "                else:\n",
    "                    Y_dev.append([0,1,0])\n",
    "            elif line[0]=='-1':\n",
    "                if if_deep==False:\n",
    "                    Y_dev.append(-1)\n",
    "                else:\n",
    "                    Y_dev.append([1,0,0])\n",
    "            else:\n",
    "                print(\"wrong!\") \n",
    "\n",
    "        for line in csv.reader(open(os.path.join(datapath,\"test.csv\"),mode='r',encoding='utf-8-sig')):\n",
    "\n",
    "            sent_vector = rep_sentencevector(line[-1],if_deep)\n",
    "            X_test.append(sent_vector)\n",
    "            if line[0] == '1':\n",
    "                if if_deep==False:\n",
    "                    Y_test.append(1)\n",
    "                else:\n",
    "                    Y_test.append([0,0,1])\n",
    "            elif line[0]=='0':\n",
    "                if if_deep==False:\n",
    "                    Y_test.append(0)\n",
    "                else:\n",
    "                    Y_test.append([0,1,0])\n",
    "            elif line[0]=='-1':\n",
    "                if if_deep==False:\n",
    "                    Y_test.append(-1)\n",
    "                else:\n",
    "                    Y_test.append([1,0,0])\n",
    "            else:\n",
    "                print(\"wrong!\")            \n",
    "    return np.array(X_train), np.array(Y_train), np.array(X_dev), np.array(Y_dev),np.array(X_test), np.array(Y_test)\n",
    "\n",
    "\n",
    "X_train, Y_train, X_dev, Y_dev,X_test,Y_test = build_traindata()\n",
    "X_train_2, Y_train_2, X_dev_2,Y_dev_2, X_test_2, Y_test_2 = build_traindata(if_deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_svm(X_train, Y_train):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='linear',probability=True)\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,\"sentiment_svm_model.m\"))\n",
    "\n",
    "\n",
    "def evaluate_svm(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1))[0])\n",
    "    for index in range(len(Y_predict)):\n",
    "        if int(Y_predict[index]) == int(Y_test[index]):\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score)) #0.8302767589196399  model accuray is :0.77675891963988\n",
    "    return score\n",
    "\n",
    "\n",
    "def predict_svm(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(rep_sentencevector(sentence1)).reshape(1, -1)\n",
    "    rep_sen2 = np.array(rep_sentencevector(sentence2)).reshape(1, -1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1)) #sentence1 [1]\n",
    "    print('sentence2', model.predict_proba(rep_sen2)) #sentence2 [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10809, 250) (10809,)\n",
      "(2679, 250) (2679,)\n",
      "(29406, 250) (29406,)\n",
      "model accuray is :0.7592385218365062\n",
      "model accuray is :0.6866965925321363\n",
      "sentence1 [[0.00178768 0.10630487 0.89190745]]\n",
      "sentence2 [[0.78767601 0.16386698 0.048457  ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "train_svm(X_train, Y_train)\n",
    "model_filepath_svm = os.path.join(model_path,'sentiment_svm_model.m')\n",
    "evaluate_svm(model_filepath_svm, X_dev, Y_dev)\n",
    "evaluate_svm(model_filepath_svm, X_test, Y_test)\n",
    "predict_svm(model_filepath_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_bayes(X_train, Y_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,\"sentiment_bayes_model.m\"))\n",
    "\n",
    "\n",
    "def evaluate_bayes(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1))[0])\n",
    "    for index in range(len(Y_predict)):\n",
    "        if int(Y_predict[index]) == int(Y_test[index]):\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score)) \n",
    "    return score\n",
    "\n",
    "\n",
    "def predict_bayes(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    " \n",
    "    rep_sen1 = np.array(rep_sentencevector(sentence1)).reshape(1, -1)\n",
    "    rep_sen2 = np.array(rep_sentencevector(sentence2)).reshape(1, -1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1))\n",
    "    print('sentence2', model.predict_proba(rep_sen2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10809, 250) (10809,)\n",
      "(2684, 250) (2684,)\n",
      "(29429, 250) (29429,)\n",
      "model accuray is :0.690387481371088\n",
      "model accuray is :0.6553399707771246\n",
      "sentence1 [[2.29578690e-15 3.08096752e-20 1.00000000e+00]]\n",
      "sentence2 [[6.91483362e-01 3.08509873e-01 6.76560446e-06]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath = os.path.join(model_path,\"sentiment_bayes_model.m\")\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "train_bayes(X_train, Y_train)\n",
    "evaluate_svm(model_filepath, X_dev, Y_dev)\n",
    "evaluate_svm(model_filepath, X_test, Y_test)\n",
    "predict_svm(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_knn(X_train, Y_train, X_test, Y_test):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    for x in range(1, 1):\n",
    "        model = KNeighborsClassifier(n_neighbors=x)\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        num = 0\n",
    "        num = 0\n",
    "        preds = preds.tolist()\n",
    "        for i, pred in enumerate(preds):\n",
    "            if int(pred) == int(Y_test[i]):\n",
    "                num += 1\n",
    "        print('K= ' + str(x) + ', precision_score:' + str(float(num) / len(preds)))\n",
    "\n",
    "    '''choose k=16 to build and train model'''\n",
    "    model = KNeighborsClassifier(n_neighbors=16)\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,\"sentiment_knn_model.m\"))\n",
    "\n",
    "\n",
    "def evaluate_knn(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1)))\n",
    "    for index in range(len(Y_predict)):\n",
    "        if Y_predict[index] == Y_test[index]:\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuray is :{0}'.format(score))\n",
    "    return score\n",
    "\n",
    "def predict_knn(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(rep_sentencevector(sentence1)).reshape(1, -1)\n",
    "    rep_sen2 = np.array(rep_sentencevector(sentence2)).reshape(1, -1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1)) \n",
    "    print('sentence2', model.predict_proba(rep_sen2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10809, 250) (10809,)\n",
      "(2684, 250) (2684,)\n",
      "(29429, 250) (29429,)\n",
      "model accuray is :0.7228017883755589\n",
      "model accuray is :0.6204424207414455\n",
      "sentence1 [[0. 0. 1.]]\n",
      "sentence2 [[0.5625 0.3125 0.125 ]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath = os.path.join(model_path,\"sentiment_knn_model.m\")\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "train_knn(X_train, Y_train, X_dev, Y_dev)\n",
    "evaluate_knn(model_filepath, X_dev, Y_dev)\n",
    "evaluate_knn(model_filepath, X_test, Y_test)\n",
    "predict_knn(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_decisiontree(X_train, Y_train):\n",
    "    from sklearn import tree\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    joblib.dump(model, os.path.join(model_path,'sentiment_decisiontree_model.m'))\n",
    "\n",
    "def evaluate_decisiontree(model_filepath, X_test, Y_test):\n",
    "    model = joblib.load(model_filepath)\n",
    "    Y_predict = list()\n",
    "    Y_test = list(Y_test)\n",
    "    right = 0\n",
    "    for sent in X_test:\n",
    "        Y_predict.append(model.predict(sent.reshape(1, -1))[0])\n",
    "    for index in range(len(Y_predict)):\n",
    "        if int(Y_predict[index]) == int(Y_test[index]):\n",
    "            right += 1\n",
    "    score = right / len(Y_predict)\n",
    "    print('model accuracy is :{0}'.format(score)) \n",
    "    return score\n",
    "\n",
    "def predict_decisiontree(model_filepath):\n",
    "    model = joblib.load(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    rep_sen1 = np.array(rep_sentencevector(sentence1)).reshape(1, -1)\n",
    "    rep_sen2 = np.array(rep_sentencevector(sentence2)).reshape(1, -1)\n",
    "    print('sentence1', model.predict_proba(rep_sen1)) \n",
    "    print('sentence2', model.predict_proba(rep_sen2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7521, 250) (7521,)\n",
      "(1865, 250) (1865,)\n",
      "(19213, 250) (19213,)\n",
      "model accuracy is :0.6026809651474531\n",
      "model accuracy is :0.5516577317441316\n",
      "sentence1 [[0. 0. 1.]]\n",
      "sentence2 [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "model_filepath = os.path.join(model_path,'sentiment_decisiontree_model.m')\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "train_decisiontree(X_train, Y_train)\n",
    "evaluate_decisiontree(model_filepath, X_dev, Y_dev)\n",
    "evaluate_decisiontree(model_filepath, X_test, Y_test)\n",
    "predict_decisiontree(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(X_train, Y_train, X_test, Y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(128, 3, activation='relu', input_shape=(seq_len, 250)))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    \n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    history=model.fit(X_train, Y_train, batch_size=100, epochs=4,shuffle=True,validation_data=(X_test, Y_test))\n",
    "    model.save(os.path.join(model_path,'sentiment_cnn_model.h5'))\n",
    "    return history\n",
    "    \n",
    "def evaluate_cnn(X_test,Y_test,model_filepath):\n",
    "    from keras.models import load_model\n",
    "    model=load_model(model_filepath)\n",
    "    loss,accuracy = model.evaluate(X_test,Y_test)\n",
    "    print('model accuracy is :{0}'.format(accuracy))\n",
    "\n",
    "    \n",
    "def predict_cnn(model_filepath):\n",
    "    from keras.models import load_model  \n",
    "    model = load_model(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    sentence_vector1 = np.array([rep_sentencevector(sentence1,if_deep=True)])\n",
    "    sentence_vector2 = np.array([rep_sentencevector(sentence2,if_deep=True)])\n",
    "    \n",
    "    print('test after load: ', model.predict(sentence_vector1))\n",
    "    print('test after load: ', model.predict(sentence_vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10809, 128, 250) (10809, 3)\n",
      "(2679, 128, 250) (2679, 3)\n",
      "(29406, 128, 250) (29406, 3)\n",
      "Train on 10809 samples, validate on 2679 samples\n",
      "Epoch 1/4\n",
      "10809/10809 [==============================] - 68s 6ms/step - loss: 0.5533 - acc: 0.7256 - val_loss: 0.4031 - val_acc: 0.8300\n",
      "Epoch 2/4\n",
      "10809/10809 [==============================] - 44s 4ms/step - loss: 0.4312 - acc: 0.8107 - val_loss: 0.3862 - val_acc: 0.8456\n",
      "Epoch 3/4\n",
      "10809/10809 [==============================] - 39s 4ms/step - loss: 0.3888 - acc: 0.8367 - val_loss: 0.3272 - val_acc: 0.8640\n",
      "Epoch 4/4\n",
      "10809/10809 [==============================] - 49s 5ms/step - loss: 0.3642 - acc: 0.8511 - val_loss: 0.3313 - val_acc: 0.8629\n",
      "25664/29406 [=========================>....] - ETA: 10s"
     ]
    }
   ],
   "source": [
    "model_filepath = os.path.join(model_path,'sentiment_cnn_model.h5')\n",
    "print(X_train_2.shape, Y_train_2.shape)\n",
    "print(X_dev_2.shape, Y_dev_2.shape)\n",
    "print(X_test_2.shape, Y_test_2.shape)\n",
    "history=train_cnn(X_train_2, Y_train_2, X_dev_2, Y_dev_2)\n",
    "evaluate_cnn(X_test_2,Y_test_2,model_filepath)\n",
    "predict_cnn(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lstm(X_train, Y_train, X_test, Y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense, Bidirectional\n",
    "    import numpy as np\n",
    "    data_dim = 250  \n",
    "    timesteps = seq_len \n",
    "    \n",
    "    '''expected input data shape: (batch_size, timesteps, data_dim)'''   \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True,\n",
    "                   input_shape=(timesteps, data_dim))))# returns a sequence of vectors of dimension 64\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True))) # returns a sequence of vectors of dimension 32\n",
    "    model.add(Bidirectional(LSTM(32)))  # return a single vector of dimension 32\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history=model.fit(X_train, Y_train, batch_size=200, epochs=4,shuffle=True,validation_data=(X_test, Y_test))\n",
    "    model.save(os.path.join(model_path,'sentiment_lstm_model.h5'))\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_lstm(X_test,Y_test,model_filepath):\n",
    "    from keras.models import load_model\n",
    "    model=load_model(model_filepath)\n",
    "    loss,accuracy = model.evaluate(X_test,Y_test)\n",
    "    print('model accuracy is :{0}'.format(accuracy))  \n",
    "\n",
    "def predict_lstm(model_filepath):\n",
    "    from keras.models import load_model  \n",
    "    model = load_model(model_filepath)\n",
    "    sentence1 = '在经营中努力为客户提供快捷优质的信息、仓储、物流、类金融等服务，利用自身资源积极拓展新的客户，同时维护与上游客户良好的关系，总体保持持续稳定的发展。'\n",
    "    sentence2 = '(3)  应收账款期末较期初减少 59,289,691.24 元，减少 35.01%，主要系本公司之子公司西藏泰达厚生医药有限公司本期销售收入下降以及整体出售原子公司四川禾正制药有限责任公司导致应收账款减少。'\n",
    "    sentence_vector1 = np.array([rep_sentencevector(sentence1,if_deep=True)])\n",
    "    sentence_vector2 = np.array([rep_sentencevector(sentence2,if_deep=True)])\n",
    "    print('test after load: ', model.predict(sentence_vector1))\n",
    "    print('test after load: ', model.predict(sentence_vector2))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10809, 128, 250) (10809, 3)\n",
      "(2687, 128, 250) (2687, 3)\n",
      "(29434, 128, 250) (29434, 3)\n",
      "Train on 10809 samples, validate on 2687 samples\n",
      "Epoch 1/4\n",
      "10809/10809 [==============================] - 214s 20ms/step - loss: 0.7566 - acc: 0.6731 - val_loss: 0.6325 - val_acc: 0.7343\n",
      "Epoch 2/4\n",
      "10809/10809 [==============================] - 202s 19ms/step - loss: 0.5593 - acc: 0.7796 - val_loss: 0.5658 - val_acc: 0.7685\n",
      "Epoch 3/4\n",
      "10809/10809 [==============================] - 202s 19ms/step - loss: 0.5006 - acc: 0.8004 - val_loss: 0.6487 - val_acc: 0.7432\n",
      "Epoch 4/4\n",
      "10809/10809 [==============================] - 205s 19ms/step - loss: 0.4668 - acc: 0.8159 - val_loss: 0.4908 - val_acc: 0.7949\n",
      "29434/29434 [==============================] - 196s 7ms/step\n",
      "model accuracy is :0.7423727661806612\n",
      "test after load:  [[0.0088307  0.10095891 0.89021033]]\n",
      "test after load:  [[0.63351744 0.27766407 0.08881846]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_filepath = os.path.join(model_path,'sentiment_lstm_model.h5')\n",
    "print(X_train_2.shape, Y_train_2.shape)\n",
    "print(X_dev_2.shape, Y_dev_2.shape)   \n",
    "print(X_test_2.shape, Y_test_2.shape)\n",
    "history_lstm=train_lstm(X_train_2, Y_train_2, X_dev_2, Y_dev_2)\n",
    "evaluate_lstm(X_test_2,Y_test_2,model_filepath)\n",
    "predict_lstm(model_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
